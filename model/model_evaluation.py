from bert_score import score


def evaluate_answers(model_answers, reference_answers):
    """
    Evaluate the answers provided by a model against the reference answers using BERTScore.

    Parameters:
        model_answers (list of str): Answers generated by the model.
        reference_answers (list of str): Ground truth answers to compare against.

    Returns:
        dict: A dictionary containing the precision, recall, and F1-score.
    """
    # Compute BERTScore
    P, R, F1 = score(model_answers, reference_answers, lang="en", rescale_with_baseline=True)

    # Calculate average precision, recall, and F1
    avg_precision = P.mean().item()
    avg_recall = R.mean().item()
    avg_f1 = F1.mean().item()

    return {
        "precision": avg_precision,
        "recall": avg_recall,
        "f1": avg_f1
    }


# Example usage
reference_answers = [
    "The primary goal of a text-to-SQL system is to transform natural language questions into structured SQL statements that accurately represent the user's intent, allowing seamless interaction with databases.",
    "The two most commonly used datasets are WikiSQL and Spider.",
    "Prompt design is critical because the inclusion of schema information and structured SQL data can significantly improve the model's performance. For example, adding database schema and SELECT statements improved Codex's execution accuracy, demonstrating that prompt components help LLMs better understand the database structure.",
    "Common types of ambiguity include lexical ambiguity (e.g., polysemy like 'Paris' as a city or person), syntactic ambiguity (e.g., 'Find all German movie directors' could mean directors of German movies or directors from Germany), semantic ambiguity (e.g., 'Are Brad and Angelina married?' could mean to each other or separately), and context-dependent ambiguity (e.g., the term 'top' may vary in meaning based on context, such as 'top movie' by ratings or 'top scorer' by goals).",
    "In-context learning allows LLMs to perform text-to-SQL tasks by providing a prompt that includes a task instruction, a natural language question, and optional examples. The LLM uses these examples to learn the mapping between NLQ and SQL on-the-fly, allowing it to generate SQL queries without being explicitly fine-tuned on a large text-to-SQL dataset. This technique leverages the model's pre-existing knowledge and can perform well, especially with strategically designed prompts.",
    "Traditional learning-based text-to-SQL methods struggle with issues like vocabulary gaps, schema ambiguity, and implicit join operations. These methods often require extensive feature engineering and are less flexible in handling diverse and complex queries. In contrast, LLM-based approaches benefit from advanced reasoning and in-context learning capabilities, which enable them to generalize across different domains and handle complex SQL generation tasks without extensive manual intervention."
]

model_answers = [
    "The primary goal of a text-to-SQL system is to translate natural language queries into structured query language (SQL) that can be executed on a database. These systems aim to automate the process of querying databases using natural language, reducing the barrier for non-experts to access and retrieve information from databases efficiently. The ultimate objective is to enable users to interact with databases in a more intuitive and user-friendly manner.",
    "The two most commonly used datasets for training and evaluating neural text-to-SQL systems are WikiSQL and Spider. These datasets are large-scale, multi-domain benchmarks that have made it possible to compare different systems easily and effectively.",
    "Prompt design significantly influences the performance of large language models (LLMs) in the text-to-SQL task. The effectiveness of prompt templates, such as DDL/SimpleDDL prefix, MD/HTML/Coding infix, and Complete/Chat postfix, impacts the model's ability to generate accurate SQL queries. Investigating unified prompt templates can help determine optimal prompt constructions and improve LLM performance in text-to-SQL tasks.",
    "Text-to-SQL systems need to handle lexical ambiguity, where a single word can have multiple meanings (e.g., \"Paris\" as a city or a person), and syntactic ambiguity, where a sentence can have multiple interpretations based on its structure (e.g., \"Find all German movie directors\" can be parsed in different ways). These types of ambiguity in natural language queries pose challenges for accurately translating them into SQL queries.",
    "In-context learning allows pretrained LLMs to perform text-to-SQL tasks by providing zero or a few training examples as demonstrations, without the need for fine-tuning. This approach leverages the advanced reasoning capabilities of LLMs to directly infer the relationship between natural language questions and SQL queries from a database. By using task instructions and test questions with corresponding databases, LLMs can demonstrate their text-to-SQL capabilities without requiring extensive fine-tuning on specific datasets.",
    "Traditional learning-based text-to-SQL methods face challenges in producing valid SQL statements due to difficulties in schema linking and skeleton parsing. They also struggle with lower accuracy rates, with the highest on the Spider leaderboard being 79.9%. In contrast, LLM-based approaches leverage advancements in LLMs for improved performance, zero-shot reasoning, and domain generalization capabilities."
]


if __name__ == "__main__":
    # Evaluate the model's answers
    results = evaluate_answers(model_answers, reference_answers)
    print(results)
